{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IAU 2023/2024\n",
    "## **Autori:** Laura Fulajtárová (50%), Bruno Kristián (50%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fáza 3 - Strojové učenie "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, PowerTransformer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OneR algoritmus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dočasne sme spojili dáta z trénovacej a testovacej množiny, aby sme mohli vytvoriť OneR model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"X_train.csv\", sep=',')\n",
    "y_train = pd.read_csv(\"y_train.csv\", sep=',')\n",
    "y_train = y_train.values.ravel()  # Converting y_train to a 1D array\n",
    "\n",
    "# Convert y_train to a DataFrame\n",
    "y_train_df = pd.DataFrame({'ack': y_train})  # Replace 'target_column_name' with the actual column name in y_train\n",
    "\n",
    "merged_train_data = pd.concat([X_train, y_train_df], axis=1)\n",
    "train_data = merged_train_data\n",
    "\n",
    "X_test = pd.read_csv(\"X_test.csv\", sep=',')\n",
    "y_test = pd.read_csv(\"y_test.csv\", sep=',')\n",
    "\n",
    "merged_test_data = pd.concat([X_test, y_test], axis=1)\n",
    "test_data = merged_test_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Rule algoritmus sme naprogramovali iba pre číselné hodnoty, pretože v dátach sme si v minulých fázach premenili pomocou encodingu všetky kategorické hodnoty na číselné."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_rule_algorithm(data, target_variable, explored_columns):\n",
    "    best_feature = None\n",
    "    best_accuracy = 0\n",
    "    \n",
    "    for feature in data.columns:\n",
    "        if feature == target_variable or feature in explored_columns:\n",
    "            continue\n",
    "        \n",
    "        mean_values = data.groupby(target_variable)[feature].mean()\n",
    "        split_point = mean_values.mean()\n",
    "        \n",
    "        temp_data = data.copy()\n",
    "        temp_data['prediction'] = temp_data[feature] > split_point\n",
    "        \n",
    "        accuracy = accuracy_score(temp_data[target_variable], temp_data['prediction'])\n",
    "        precision = precision_score(temp_data[target_variable], temp_data['prediction'])\n",
    "        recall = recall_score(temp_data[target_variable], temp_data['prediction'])\n",
    "        \n",
    "        if accuracy > best_accuracy:\n",
    "            best_feature = feature\n",
    "            best_accuracy = accuracy\n",
    "            best_precision = precision\n",
    "            best_recall = recall\n",
    "            result = temp_data['prediction']\n",
    "    \n",
    "    return best_feature, best_accuracy, best_precision, best_recall, result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spustíme algoritmus OneR s predikovanou premennou \"ack\" a vyhodnotíme metriky."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na vyhodnotenie modelu sme použili nasledovné metriky:\n",
    "- Accuracy - ako často klasiﬁkátor správne klasifikoval \n",
    "\n",
    "- Precision - koľko správne predikovaných príkladov bolo pozitívnych\n",
    "\n",
    "- Recall - koľko pozitívnych príkladov bolo správne predikovaných"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OneR s jednou premennou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variable = 'ack'\n",
    "\n",
    "oneR_train_data = copy.deepcopy(train_data)\n",
    "oneR_test_data = copy.deepcopy(test_data)\n",
    "\n",
    "best_feature, best_accuracy, best_precision, best_recall, prediction = one_rule_algorithm(oneR_train_data, target_variable, [])\n",
    "print('Best column from OneR:', best_feature)\n",
    "print('Accuracy:', best_accuracy)\n",
    "print('\\n')\n",
    "\n",
    "oneR_train_data[[best_feature, 'ack']]\n",
    "best_feature, best_accuracy, best_precision, best_recall, prediction = one_rule_algorithm(oneR_test_data, target_variable, [])\n",
    "print('Best column from OneR:', best_feature)\n",
    "print('Accuracy:', best_accuracy)\n",
    "print('Precision:', best_precision)\n",
    "print('Recall:', best_recall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vidíme, že algoritmus vyhodnotil \"page_activity_duration\" ako najlepšiu premennú pre predikciu \"ack\". Algoritmus na základe premennej \"page_activity_duration\" predikoval s presnosťou takmer 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.kdeplot(train_data[train_data['ack'] == 0][best_feature], label='ack = 0')\n",
    "sns.kdeplot(train_data[train_data['ack'] == 1][best_feature], label='ack = 1')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OneR s viacerými premennými"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz skúsime spustiť OneR algoritmus viackrát a zistiť ktoré premenné najlepšie predikujú \"ack\".\n",
    "\n",
    "Po zistení najlepších premenných ich použijeme na vytvorenie modelu. Pri vytváraní modelu prihliadame na presnosť predikcie danej premennej. Čím vyššia presnosť, tým má vyššiu váhu pri rozhodaovaní. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variable = 'ack'\n",
    "results = []\n",
    "explored_columns = []\n",
    "final_results = []\n",
    "\n",
    "for _ in range(5):\n",
    "    oneR_train_data = copy.deepcopy(train_data)\n",
    "    best_feature, best_accuracy, best_precision, best_recall, prediction = one_rule_algorithm(oneR_train_data, target_variable, explored_columns)\n",
    "    explored_columns.append(best_feature)\n",
    "    results.append((best_accuracy, prediction.tolist()))\n",
    "\n",
    "for result in results:\n",
    "    if result[0] > 0.7:\n",
    "        final_results.append(result[1])\n",
    "        final_results.append(result[1])\n",
    "    else:\n",
    "        final_results.append(result[1])\n",
    "\n",
    "combined_predictions = [Counter(sample).most_common(1)[0][0] for sample in zip(*final_results)]\n",
    "\n",
    "overall_accuracy = accuracy_score(train_data[target_variable], combined_predictions)\n",
    "overall_precision = precision_score(train_data[target_variable], combined_predictions, zero_division=0)\n",
    "overall_recall = recall_score(train_data[target_variable], combined_predictions)\n",
    "\n",
    "table_data = {\n",
    "    \"Algorithm\": [\"OneR\"],\n",
    "    \"Accuracy\": [overall_accuracy],\n",
    "    \"Precision\": [overall_precision],\n",
    "    \"Recall\": [overall_recall]\n",
    "}\n",
    "\n",
    "oneR_table = pd.DataFrame(table_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Najlepšie premenné k predikcií \"ack\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_data = {\n",
    "    \"Feature\": explored_columns,\n",
    "    \"Accuracy\": [result[0] for result in results]\n",
    "}\n",
    "\n",
    "accuracy_table = pd.DataFrame(table_data)\n",
    "accuracy_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Výsledky metrík pre predikovanie premennej \"ack\" pomocou OneR algoritmu s viacerými premennými."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Overall Accuracy:', overall_accuracy)\n",
    "print('Overall Precision:', overall_precision)\n",
    "print('Overall Recall:', overall_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learn Klasifikátory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stromový klasifikátor Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Analysis for varying model complexity\n",
    "results = []\n",
    "max_depth_range = range(1, 15)\n",
    "\n",
    "for depth in max_depth_range:\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, max_depth=depth, random_state=1)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Calculate error on training set\n",
    "    train_pred = rf_model.predict(X_train)\n",
    "    train_error = 1 - accuracy_score(y_train, train_pred)\n",
    "\n",
    "    # Calculate error on test set\n",
    "    test_pred = rf_model.predict(X_test)\n",
    "    test_error = 1 - accuracy_score(y_test, test_pred)\n",
    "\n",
    "    results.append({'max_depth': depth, 'train_error': train_error, 'test_error': test_error})\n",
    "\n",
    "# Convert results to a DataFrame for easier plotting\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Find the max_depth with the lowest test error\n",
    "optimal_depth = results_df['max_depth'][results_df['test_error'].idxmin()]\n",
    "\n",
    "# Plotting the errors vs model complexity\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(results_df['max_depth'], results_df['train_error'], label='Training Error', marker='o', color='blue')\n",
    "plt.plot(results_df['max_depth'], results_df['test_error'], label='Testing Error', marker='o', color='red')\n",
    "\n",
    "# Add vertical line at the point of lowest test error\n",
    "plt.axvline(x=optimal_depth, color='green', linestyle='--', label=f'Optimal Depth: {optimal_depth}')\n",
    "\n",
    "plt.title(\"Error vs. Model Complexity (Max Depth)\")\n",
    "plt.xlabel(\"Max Depth\")\n",
    "plt.ylabel(\"Error (1 - Accuracy)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Display the optimal depth and corresponding errors\n",
    "optimal_info = results_df.loc[results_df['max_depth'] == optimal_depth]\n",
    "optimal_info\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "estimators = []\n",
    "\n",
    "clf = RandomForestClassifier(max_depth = 17, min_samples_split = 6, min_samples_leaf = 4)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "predictions = clf.predict(X_test)\n",
    "train_predictions = clf.predict(X_train)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "accuracy_train = accuracy_score(y_train, train_predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "\n",
    "print(f\"Accuracy Train: {accuracy_train:.2f}\")\n",
    "print('\\n')\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "\n",
    "table_data = {\n",
    "    \"Algorithm\": [\"Random Forest\"],\n",
    "    \"Accuracy\": [accuracy],\n",
    "    \"Precision\": [precision],\n",
    "    \"Recall\": [recall]\n",
    "}\n",
    "\n",
    "RF_table = pd.DataFrame(table_data)\n",
    "\n",
    "feature_importances = clf.feature_importances_\n",
    "sorted_indices = np.argsort(feature_importances)[::-1]\n",
    "top_5_features_rf = sorted_indices[:5]\n",
    "top_5_importances = feature_importances[top_5_features_rf]\n",
    "\n",
    "feature_names = X_test.columns.values\n",
    "\n",
    "top_5_feature_names_rf = [feature_names[i] for i in top_5_features_rf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stromový klasifikátor Gradient Boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Analysis for varying model complexity\n",
    "results = []\n",
    "max_depth_range = range(1, 15) \n",
    "\n",
    "for depth in max_depth_range:\n",
    "    gb_model = GradientBoostingClassifier(max_depth=depth)\n",
    "    gb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Calculate error on training set\n",
    "    train_pred = gb_model.predict(X_train)\n",
    "    train_error = 1 - accuracy_score(y_train, train_pred)\n",
    "\n",
    "    # Calculate error on test set\n",
    "    test_pred = gb_model.predict(X_test)\n",
    "    test_error = 1 - accuracy_score(y_test, test_pred)\n",
    "\n",
    "    results.append({'max_depth': depth, 'train_error': train_error, 'test_error': test_error})\n",
    "\n",
    "# Convert results to a DataFrame for easier plotting\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Find the max_depth with the lowest test error\n",
    "optimal_depth = results_df['max_depth'][results_df['test_error'].idxmin()]\n",
    "\n",
    "# Plotting the errors vs model complexity\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(results_df['max_depth'], results_df['train_error'], label='Training Error', marker='o', color='blue')\n",
    "plt.plot(results_df['max_depth'], results_df['test_error'], label='Testing Error', marker='o', color='red')\n",
    "\n",
    "# Add vertical line at the point of lowest test error\n",
    "plt.axvline(x=optimal_depth, color='green', linestyle='--', label=f'Optimal Depth: {optimal_depth}')\n",
    "\n",
    "plt.title(\"Error vs. Model Complexity (Max Depth)\")\n",
    "plt.xlabel(\"Max Depth\")\n",
    "plt.ylabel(\"Error (1 - Accuracy)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Display the optimal depth and corresponding errors\n",
    "optimal_info = results_df.loc[results_df['max_depth'] == optimal_depth]\n",
    "optimal_info\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model = GradientBoostingClassifier(n_estimators=100, random_state=1, max_depth = 3)\n",
    "\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "gb_y_pred = gb_model.predict(X_test)\n",
    "gb_y_pred_train = gb_model.predict(X_train)\n",
    "\n",
    "gb_trin_accuracy = accuracy_score(y_train, gb_y_pred_train)\n",
    "gb_accuracy = accuracy_score(y_test, gb_y_pred)\n",
    "gb_precision = precision_score(y_test, gb_y_pred)\n",
    "gb_recall = recall_score(y_test, gb_y_pred)\n",
    "\n",
    "print(f\"Accuracy Train: {gb_trin_accuracy:.2f}\")\n",
    "print('\\n')\n",
    "print(f\"Accuracy: {gb_accuracy:.2f}\")\n",
    "print(f\"Precision: {gb_precision:.2f}\")\n",
    "print(f\"Recall: {gb_recall:.2f}\")\n",
    "\n",
    "table_data = {\n",
    "    \"Algorithm\": [\"Gradient Boosting\"],\n",
    "    \"Accuracy\": [gb_accuracy],\n",
    "    \"Precision\": [gb_precision],\n",
    "    \"Recall\": [gb_recall]\n",
    "}\n",
    "\n",
    "GB_table = pd.DataFrame(table_data)\n",
    "\n",
    "feature_importances = gb_model.feature_importances_\n",
    "sorted_indices = np.argsort(feature_importances)[::-1]\n",
    "top_5_features_gb = sorted_indices[:5]\n",
    "top_5_importances = feature_importances[top_5_features_gb]\n",
    "feature_names = X_test.columns.values\n",
    "\n",
    "top_5_feature_names = [feature_names[i] for i in top_5_features_gb]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max depth by nám stačilo nastaviť na hĺbku 3, pretože pri vyšších hĺbkach sa model začal pretrénovať."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nestromový algoritmus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre nestromový algoritmus sme si vybrali KNN klasifikátor.\n",
    "\n",
    "Najskôr si dáta scalujeme a normalizujeme, aby KNN klasifikátor pracoval efektívnejšie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(X_train)\n",
    "train_data_scaled = pd.DataFrame(scaled_data, columns=X_train.columns)\n",
    "\n",
    "power = PowerTransformer(method='yeo-johnson', standardize=True) \n",
    "X_train_normalised = power.fit_transform(train_data_scaled)\n",
    "X_train_scaled_normalised = pd.DataFrame(X_train_normalised, columns=X_train.columns)\n",
    "\n",
    "scaled_test_data = scaler.transform(X_test)\n",
    "test_data_scaled = pd.DataFrame(scaled_test_data, columns=X_test.columns)\n",
    "\n",
    "normalized_test_data = power.transform(test_data_scaled)\n",
    "test_data_scaled_normalized = pd.DataFrame(normalized_test_data, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pri stromových klasifikačných algoritmoch sme nemuseli použiť feature selection, pretože si s tým poradili samé. Pri nestromových algoritmoch je to však inak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = [\n",
    "    'page_activity_duration', 'pct_doubleclick', 'pct_mouse_click', 'wild_mouse_duration',\n",
    "    'pct_mouse_move', 'pct_input', 'pct_click', 'session_start', \n",
    "    'scroll_move_total_rel_distance', 'pct_scroll_move'\n",
    "]\n",
    "\n",
    "X_train_filtered = X_train_scaled_normalised[columns_to_keep]\n",
    "X_test_filtered = test_data_scaled_normalized[columns_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN klasifikátor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model = KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "knn_model.fit(X_train_filtered, y_train)\n",
    "\n",
    "knn_y_pred = knn_model.predict(X_test_filtered)\n",
    "knn_y_pred_train = knn_model.predict(X_train_filtered)\n",
    "\n",
    "knn_trin_accuracy = accuracy_score(y_train, knn_y_pred_train)\n",
    "knn_accuracy = accuracy_score(y_test, knn_y_pred)\n",
    "knn_precision = precision_score(y_test, knn_y_pred)\n",
    "knn_recall = recall_score(y_test, knn_y_pred)\n",
    "\n",
    "print(f\"Accuracy Train: {knn_trin_accuracy:.2f}\")\n",
    "print('\\n')\n",
    "print(f\"Accuracy: {knn_accuracy:.2f}\")\n",
    "print(f\"Precision: {knn_precision:.2f}\")\n",
    "print(f\"Recall: {knn_recall:.2f}\")\n",
    "\n",
    "table_data_knn = {\n",
    "    \"Algorithm\": [\"K-Nearest Neighbors\"],\n",
    "    \"Accuracy\": [knn_accuracy],\n",
    "    \"Precision\": [knn_precision],\n",
    "    \"Recall\": [knn_recall]\n",
    "}\n",
    "\n",
    "knn_table = pd.DataFrame(table_data_knn)\n",
    "\n",
    "\n",
    "selector = SelectKBest(score_func=f_classif, k=5)\n",
    "selector.fit(X_train_filtered, y_train)\n",
    "top_5_indices = selector.get_support(indices=True)\n",
    "top_5_features_knn = X_train_filtered.columns[top_5_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"results = []\n",
    "neighbours = range(1, 15)\n",
    "\n",
    "for neighbour in neighbours:\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=neighbour)\n",
    "    knn_model.fit(X_train_filtered, y_train)\n",
    "    \n",
    "    # Calculate error on training set\n",
    "    train_pred = knn_model.predict(X_train_filtered)\n",
    "    train_error = 1 - accuracy_score(y_train, train_pred)\n",
    "\n",
    "    # Calculate error on test set\n",
    "    test_pred = knn_model.predict(X_test_filtered)\n",
    "    test_error = 1 - accuracy_score(y_test, test_pred)\n",
    "\n",
    "    results.append({'n_neighbors': neighbour, 'train_error': train_error, 'test_error': test_error})\n",
    "\n",
    "# Convert results to a DataFrame for easier plotting\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Find the number of neighbors with the lowest test error\n",
    "optimal_neighbors = results_df['n_neighbors'][results_df['test_error'].idxmin()]\n",
    "\n",
    "# Plotting the errors vs model complexity\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(results_df['n_neighbors'], results_df['train_error'], label='Training Error', marker='o')\n",
    "plt.plot(results_df['n_neighbors'], results_df['test_error'], label='Testing Error', marker='o')\n",
    "\n",
    "# Add vertical line at the point of lowest test error\n",
    "plt.axvline(x=optimal_neighbors, color='green', linestyle='--', label=f'Optimal Neighbors: {optimal_neighbors}')\n",
    "\n",
    "plt.title(\"Error vs. Model Complexity (Number of Neighbors)\")\n",
    "plt.xlabel(\"Number of Neighbors\")\n",
    "plt.ylabel(\"Error (1 - Accuracy)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Display the optimal number of neighbors and corresponding errors\n",
    "optimal_info = results_df.loc[results_df['n_neighbors'] == optimal_neighbors]\n",
    "print(optimal_info)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Porovnanie výsledkov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(oneR_table, RF_table, GB_table, knn_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z porovnania výsledkov vidíme, že najlepšie výsledky dosiahol Gradient Boosting klasifikátor.\n",
    "\n",
    "Náš oneR algoritmus dosiahol presnosť 83%. Dôvodom je jednoduchosť algoritmu a potreba predikovať pomocou viacerých premenných. Keďže najlepší atribút má presnosť okolo 90% a ostané atribúty majú presnosť len okolo 60%, tak pri predikovaní pomocou viacerých atribútov sa presnosť zníži.\n",
    "\n",
    "OneR algoritmus predikuje s precíznosťou 82%, čo je o viac ako 10% menej ako Gradient Boosting klasifikátor alebo Random Forest klasifikátor.\n",
    "\n",
    "Náš algoritmus je o 15% horší ako ako Gradient Boosting klasifikátor alebo Random Forest klasifikátor v predikovaní pozitívnych príkladov zo všetkých pozitívnych príkladov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"One Rule Algorithm:\")\n",
    "for column in explored_columns:\n",
    "    print(column)\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Random Forest:\")\n",
    "for column in top_5_feature_names_rf:\n",
    "    print(column)\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Gradient Boosting:\")\n",
    "for column in top_5_feature_names:\n",
    "    print(column)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"K Nearest Neighbours:\")\n",
    "for column in top_5_features_knn:\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vidíme, že algoritmy sa podobajú vo výbere atribútov, ktoré najlepšie predikujú \"ack\". Najlepšie atribúty sú \"page_activity_duration\", \"pct_doubleclick\" a \"pct_click\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vizualizácia natrénovaných pravidiel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vizualizujeme si Random Forest klasifikátor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_index = 0\n",
    "plt.figure(figsize=(12, 8))\n",
    "plot_tree(clf.estimators_[tree_index], filled=True, feature_names=[f\"feature_{i}\" for i in range(X_test.shape[1])])\n",
    "plt.title(f\"Decision Tree {tree_index + 1} from Random Forest Classifier\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vizualizácia pravidiel pre Random Forest klasifikátor na základe accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"feature_accuracies = []\n",
    "\n",
    "for i in range(X_train.shape[1]):\n",
    "    X_train_single_feature = X_train.iloc[:, i:i+1]  \n",
    "    X_test_single_feature = X_test.iloc[:, i:i+1]   \n",
    "    \n",
    "    clf_single_feature = RandomForestClassifier()\n",
    "    clf_single_feature.fit(X_train_single_feature, y_train)  \n",
    "    \n",
    "    predictions_single_feature = clf_single_feature.predict(X_test_single_feature)\n",
    "    \n",
    "    accuracy_single_feature = accuracy_score(y_test, predictions_single_feature)\n",
    "    feature_accuracies.append((X_train.columns[i], accuracy_single_feature))\n",
    "\n",
    "sorted_features = sorted(feature_accuracies, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "top_5_feature_names = [feat[0] for feat in sorted_features[:5]]\n",
    "top_5_accuracies = [feat[1] for feat in sorted_features[:5]]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(top_5_feature_names, top_5_accuracies, color='skyblue')\n",
    "plt.xlabel('Feature Name')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Top 5 Features with Accuracies')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop other columns\n",
    "X_train_filtered = X_train[top_5_feature_names]\n",
    "X_test_filtered = X_test[top_5_feature_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting je veľmi presný model, obzvlášť účinný pri práci s komplexnými dátami, čo je výhodné v rôznych aplikáciách od klasifikácie po regresiu. Jeho flexibilita umožňuje prispôsobenie rozličným typom stratových funkcií, čo ho robí vhodným pre rôzne prediktívne úlohy. Okrem toho, ponúka viacero možností na kontrolu overfittingu, ako napríklad nastavenie hĺbky stromu a rýchlosti učenia, čo zlepšuje jeho schopnosť generalizovať na nevidené dáta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vysvetlenie hyperparametrov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = GradientBoostingClassifier().get_params()\n",
    "for key, value in params.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ccp_alpha: Komplexita stromu. Čím vyššia hodnota, tým viac sa strom zjednoduší.\n",
    "- criterion: Kritérium pre výber atribútu, ktorý sa použije na rozdelenie uzla.\n",
    "- init: Inicializácia stromu.\n",
    "- learning_rate: Rýchlosť učenia.\n",
    "- loss: Funkcia straty.\n",
    "- max_depth: Maximálna hĺbka stromu.\n",
    "- max_features: Maximálny počet atribútov, ktoré sa použijú na rozdelenie uzla.\n",
    "- max_leaf_nodes: Maximálny počet listových uzlov.\n",
    "- min_impurity_decrease: Uzol sa rozdelí, ak sa impurity v jeho rodičovskom uzle zníži o túto hodnotu.\n",
    "- min_samples_leaf: Minimálny počet vzoriek v listovom uzle.\n",
    "- min_weight_fraction_leaf: Minimálna váhová frakcia v listovom uzle.\n",
    "- n_estimators: Počet stromov v ensemble.\n",
    "- n_iter_no_change: Počet iterácií bez zlepšenia pred ukončením učenia.\n",
    "- random_state: Seed pre generovanie náhodných čísel.\n",
    "- subsample: Podiel vzoriek použitých na trénovanie každého stromu.\n",
    "- tol: Tolerancia pre zastavenie učenia.\n",
    "- validation_fraction: Podiel vzoriek použitých na validáciu každého stromu.\n",
    "- verbose: Výpis informácií o učení.\n",
    "- warm_start: Použiť existujúci model na trénovanie a pridať ďalšie stromy do ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted parameter grid to combat overfitting\n",
    "gb_parameters = {\n",
    "    'criterion': ['friedman_mse', 'squared_error'],\n",
    "    'loss': ['log_loss', 'exponential'],\n",
    "    'max_depth': [1, 2, 3, 4],\n",
    "    'n_estimators': [80, 90, 100, 110, 120],\n",
    "    'learning_rate': [0.09, 0.1, 0.11],\n",
    "    'subsample': [0.8, 0.9, 1.0],  # Adding subsample parameter\n",
    "    'min_samples_split': [3, 4, 5],\n",
    "    'min_samples_leaf': [2, 3, 4],\n",
    "    'random_state': [None, 1]\n",
    "}\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "gb_clf = RandomizedSearchCV(\n",
    "    estimator=GradientBoostingClassifier(),\n",
    "    param_distributions=gb_parameters,\n",
    "    cv=5,\n",
    "    n_iter=10,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the random search\n",
    "gb_search = gb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Output the results\n",
    "print(\"Best Estimator:\", gb_search.best_estimator_)\n",
    "print(\"Best Score:\", gb_search.best_score_)\n",
    "print(\"Best Parameters:\", gb_search.best_params_)\n",
    "print(\"Number of splits:\", gb_search.n_splits_)\n",
    "\n",
    "# Train the best model with early stopping\n",
    "best_gb_model = gb_search.best_estimator_\n",
    "best_gb_model.n_iter_no_change = 5\n",
    "best_gb_model.validation_fraction = 0.1\n",
    "best_gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Feature importances and reduction\n",
    "feature_importances = best_gb_model.feature_importances_\n",
    "N = 10  # Top N features\n",
    "top_features_indices = feature_importances.argsort()[-N:][::-1]\n",
    "top_features = [X_train.columns[i] for i in top_features_indices]\n",
    "X_train_reduced = X_train[top_features]\n",
    "X_test_reduced = X_test[top_features]\n",
    "\n",
    "# Train and evaluate on the reduced feature set\n",
    "final_model = best_gb_model.fit(X_train_reduced, y_train)\n",
    "final_model_score = final_model.score(X_test_reduced, y_test)\n",
    "print(\"Final Model Score on Test Data:\", final_model_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient Boosting before hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model = GradientBoostingClassifier()\n",
    "\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "gb_y_pred = gb_model.predict(X_test)\n",
    "\n",
    "gb_accuracy = accuracy_score(y_test, gb_y_pred)\n",
    "gb_precision2 = precision_score(y_test, gb_y_pred)\n",
    "gb_recall2 = recall_score(y_test, gb_y_pred)\n",
    "\n",
    "print(f\"Accuracy: {gb_accuracy:.5f}\")\n",
    "print(f\"Precision: {gb_precision2:.5f}\")\n",
    "print(f\"Recall: {gb_recall2:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_performance(clf, X_train, y_train, X_test, y_test):\n",
    "    # Fit the model\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions on training and test data\n",
    "    train_predictions = clf.predict(X_train)\n",
    "    test_predictions = clf.predict(X_test)\n",
    "\n",
    "    # Calculate metrics for training data\n",
    "    train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "    train_precision = precision_score(y_train, train_predictions)\n",
    "    train_recall = recall_score(y_train, train_predictions)\n",
    "\n",
    "    # Calculate metrics for test data\n",
    "    test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "    test_precision = precision_score(y_test, test_predictions)\n",
    "    test_recall = recall_score(y_test, test_predictions)\n",
    "\n",
    "    # Print training metrics\n",
    "    print(\"Training Metrics:\")\n",
    "    print(f\"Accuracy: {train_accuracy:.5f}\")\n",
    "    print(f\"Precision: {train_precision:.5f}\")\n",
    "    print(f\"Recall: {train_recall:.5f}\\n\")\n",
    "\n",
    "    # Print test metrics\n",
    "    print(\"Test Metrics:\")\n",
    "    print(f\"Accuracy: {test_accuracy:.5f}\")\n",
    "    print(f\"Precision: {test_precision:.5f}\")\n",
    "    print(f\"Recall: {test_recall:.5f}\\n\")\n",
    "\n",
    "    # Check for Underfitting\n",
    "    if train_accuracy < 0.6 and test_accuracy < 0.6:\n",
    "        print(\"The model is underfitting.\")\n",
    "    else:\n",
    "        print(f\"The model is not underfitting because the training and test accuracies are {train_accuracy:.5f} and {test_accuracy:.5f}.\")\n",
    "\n",
    "    # Check for Overfitting\n",
    "    if train_accuracy > 0.8 and (train_accuracy - test_accuracy) > 0.1 or train_accuracy == 1.0:\n",
    "        print(\"The model is overfitting.\")\n",
    "    else:\n",
    "        print(f\"The model is not overfitting because the difference between the training and test accuracies is {train_accuracy - test_accuracy:.5f}.\")\n",
    "\n",
    "    # Check for Stability\n",
    "    if abs(train_accuracy - test_accuracy) < 0.1:\n",
    "        print(f\"The model is stable because the difference between the training and test accuracies is {abs(train_accuracy - test_accuracy):.5f}.\")\n",
    "    else:\n",
    "        print(\"The model is not stable.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient Boosting after hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = gb_search.best_estimator_\n",
    "evaluate_model_performance(clf, X_train_reduced, y_train, X_test_reduced, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest je obľúbený pre svoju robustnosť a výkonnosť, ktorá je často porovnateľná s komplexnejšími modelmi. Vďaka svojmu spôsobu náhodnej výberu prvkov a tvorby stromov poskytuje Random Forest vynikajúcu odolnosť voči overfittingu. Táto metóda je tiež schopná efektívne spracovávať veľké dátove sady s veľkým počtom premenných, čo je výhodné pri riešení širokej škály úloh prediktívnej analýzy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vysvetlenie hyperparametrov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = RandomForestClassifier().get_params()\n",
    "for key, value in params.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- bootstrap: Použiť bootstrap vzorkovanie. Bootstrap vzorkovanie je metóda, ktorá sa používa na zlepšenie presnosti modelu.\n",
    "- class_weight: Váhy tried.\n",
    "- max_samples: Maximálny počet vzoriek použitých na trénovanie každého stromu.\n",
    "-  n_jobs: Počet jadier procesora použitých na trénovanie.\n",
    "-  oob_score: Použiť out-of-bag vzorky na odhad presnosti. Out-of-bag vzorky sú vzorky, ktoré neboli použité na trénovanie stromu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"estimator = RandomForestClassifier()\n",
    "\n",
    "# Random Forest Parameters\n",
    "rf_parameters = {\n",
    "    'n_estimators': [80,90,100,110,120],\n",
    "    'max_depth': [ 3,4,5,6,7],\n",
    "    'min_samples_split': [2, 3,4],\n",
    "    'min_samples_leaf': [1, 2,3,4],\n",
    "    'bootstrap': [True, False],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "scoring = ['accuracy', 'precision_micro']\n",
    "\n",
    "rf_clf = RandomizedSearchCV(\n",
    "    estimator=estimator, \n",
    "    param_distributions=rf_parameters, \n",
    "    cv=5, \n",
    "    n_jobs=-1,\n",
    "    n_iter=10,\n",
    ")\n",
    "\n",
    "rf_search = rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Output the results\n",
    "print(\"Best Estimator:\", rf_search.best_estimator_)\n",
    "print(\"Best Score:\", rf_search.best_score_)\n",
    "print(\"Best Parameters:\", rf_search.best_params_)\n",
    "print(\"Number of Splits:\", rf_search.n_splits_)\n",
    "\n",
    "# Feature Importance and Selection\n",
    "best_rf_model = rf_search.best_estimator_.fit(X_train, y_train)\n",
    "rf_feature_importances = best_rf_model.feature_importances_\n",
    "\n",
    "# Select the top N features (e.g., top 5)\n",
    "N = 5\n",
    "rf_top_features_indices = rf_feature_importances.argsort()[-N:][::-1]\n",
    "rf_top_features = [X_train.columns[i] for i in rf_top_features_indices]\n",
    "\n",
    "# Restrict your training and testing sets to these top features\n",
    "X_train_reduced = X_train[rf_top_features]\n",
    "X_test_reduced = X_test[rf_top_features]\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest before hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "predictions = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "precision = precision_score(y_test, predictions)\n",
    "recall = recall_score(y_test, predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.5f}\")\n",
    "print(f\"Precision: {precision:.5f}\")\n",
    "print(f\"Recall: {recall:.5f}\")\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest after hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"clf = rf_search.best_estimator_\n",
    "evaluate_model_performance(clf, X_train_reduced, y_train, X_test_reduced, y_test)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNeighbors classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors (KNN) je známy svojou jednoduchosťou a intuitívnosťou, čo uľahčuje jeho pochopenie a implementáciu. Jeho flexibilita výberu počtu susedov a metrík vzdialenosti umožňuje prispôsobiť model špecifickým potrebám dát. Navyše, KNN je efektívny pri zachytávaní komplexných vzorov v dátach, čo je výhodné, keď sú presné predpovede založené na podobnosti dát kľúčové."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vysvetlenie hyperparametrov\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = KNeighborsClassifier().get_params()\n",
    "for key, value in params.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- algorithm: Algoritmus použitý na nájdenie najbližších susedov. Algoritmus môže byť \"ball_tree\", \"kd_tree\", \"brute\" alebo \"auto\".\n",
    "- leaf_size: Veľkosť listového uzla.\n",
    "- metric: Metrika použitá na výpočet vzdialenosti medzi bodmi. Metrika môže byť \"euclidean\", \"manhattan\" alebo \"minkowski\".\n",
    "- n_neighbors: Počet najbližších susedov.\n",
    "- p: Parameter pre metriku \"minkowski\".\n",
    "- weights: Váhy použité na predikciu bodu. Váhy môžu byť \"uniform\" alebo \"distance\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# KNN Parameters\n",
    "knn_parameters = {\n",
    "    'n_neighbors': [5, 6, 7, 8, 9, 10],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'leaf_size': [20, 25, 30, 35, 40],\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski'],\n",
    "    'p': [1, 2]\n",
    "}\n",
    "\n",
    "scoring = ['accuracy', 'precision_micro']\n",
    "\n",
    "knn_clf = RandomizedSearchCV(\n",
    "    estimator=KNeighborsClassifier(),\n",
    "    param_distributions=knn_parameters,\n",
    "    scoring=scoring,\n",
    "    refit='accuracy',\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "knn_search = knn_clf.fit(X_train_reduced, y_train)\n",
    "\n",
    "# Output the results\n",
    "print(\"Best Estimator:\", knn_search.best_estimator_)\n",
    "print(\"Best Score:\", knn_search.best_score_)\n",
    "print(\"Best Parameters:\", knn_search.best_params_)\n",
    "print(\"Number of Splits:\", knn_search.n_splits_)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KNN before hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"knn_model = KNeighborsClassifier()\n",
    "\n",
    "knn_model.fit(X_train_filtered, y_train)\n",
    "\n",
    "knn_y_pred = knn_model.predict(X_test_filtered)\n",
    "\n",
    "knn_accuracy = accuracy_score(y_test, knn_y_pred)\n",
    "knn_precision = precision_score(y_test, knn_y_pred)\n",
    "knn_recall = recall_score(y_test, knn_y_pred)\n",
    "\n",
    "print(f\"Accuracy: {knn_accuracy:.5f}\")\n",
    "print(f\"Precision: {knn_precision:.5f}\")\n",
    "print(f\"Recall: {knn_recall:.5f}\")\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KNN after hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"clf = knn_search.best_estimator_\n",
    "evaluate_model_performance(clf, X_train_reduced, y_train, X_test_reduced, y_test)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Voting ensemble "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Create individual models with their best parameters\n",
    "model1 = gb_search.best_estimator_\n",
    "model2 = rf_search.best_estimator_\n",
    "model3 = knn_search.best_estimator_\n",
    "\n",
    "# Create a voting classifier\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('gb', model1), ('rf', model2), ('knn', model3)],\n",
    "    voting='hard'\n",
    ")\n",
    "\n",
    "# Fit the voting classifier\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "voting_score = voting_clf.score(X_test, y_test)\n",
    "print(f\"Voting Classifier Score: {voting_score}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"clf = voting_clf\n",
    "evaluate_model_performance(clf, X_train_reduced, y_train, X_test_reduced, y_test)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacking ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Create individual models with their best parameters\n",
    "model1 = gb_search.best_estimator_\n",
    "model2 = rf_search.best_estimator_\n",
    "model3 = knn_search.best_estimator_\n",
    "\n",
    "# Define the stacking ensemble\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=[('gb', model1), ('rf', model2), ('knn', model3)],\n",
    "    final_estimator=LogisticRegression(),\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "# Fit the stacking classifier\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "stacking_score = stacking_clf.score(X_test, y_test)\n",
    "print(f\"Stacking Classifier Score: {stacking_score}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"clf = stacking_clf\n",
    "evaluate_model_performance(clf, X_train_reduced, y_train, X_test_reduced, y_test)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Voľba najlepšieho modelu\n",
    "Pre nasadenie modelu sme si vybrali Gradient Boosting klasifikátor, pretože dosiahol najlepšie výsledky. Nevykazoval ani overfitting, ani underfitting a bol stabilný. V porovnaní s ostatnými modelmi dosiahol najlepšie výsledky."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vyhodnotenie vplyvu zvolenej stratégie riešenia na klasifikáciu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning resp. ensemble learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "robilo sa v Laurinej casti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Najlepší model pre nasadenie (deployment)? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data pipeline pre jeho vybudovanie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Najlepší model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Načítame nespracované dáta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = pd.read_csv(\"user_phase_1.csv\", sep='\\t')\n",
    "session = pd.read_csv(\"session_phase_1.csv\", sep='\\t')\n",
    "data= pd.merge(user, session, on='user_id', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rozdelíme si ich na trénovaciu a testovaciu množinu, dávame si pozor na rovnomernú distribúciu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data.drop('ack', axis=1)\n",
    "y = data['ack']  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "train_target_distribution = y_train.value_counts(normalize=True)\n",
    "test_target_distribution = y_test.value_counts(normalize=True)\n",
    "\n",
    "# Compare distributions\n",
    "print(\"Train Target Distribution:\")\n",
    "print(train_target_distribution)\n",
    "print(\"\\nTest Target Distribution:\")\n",
    "print(test_target_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    return accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vytvoríme si funkciu na inputation of missing variables a na encoding kategorických premenných. Vo funkcii taktiež detekujeme outlierov a nahrádzame ich priemerom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "columns_to_drop = ['sex', 'race', 'browser_name']\n",
    "\n",
    "numeric_columns = [\n",
    "    'pct_scroll_move', \n",
    "    'pct_doubleclick', \n",
    "    'pct_input', \n",
    "    'page_activity_duration', \n",
    "    'pct_click',\n",
    "    'pct_mouse_move',\n",
    "    'pct_scrandom', \n",
    "    'pct_scroll_move_duration', \n",
    "    'mouse_move_total_rel_distance',\n",
    "    'pct_rage_click',\n",
    "    'pct_wild_mouse', \n",
    "    'wild_mouse_duration', \n",
    "    'pct_click_product_info', \n",
    "    'scroll_move_total_rel_distance',\n",
    "    'pct_mouse_click'\n",
    "]\n",
    "\n",
    "object_columns = [\n",
    "    'birthdate',\n",
    "    'registration',\n",
    "    'session_start'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_and_outliers(X):\n",
    "    A = X.copy()\n",
    "    A = A.drop(columns=columns_to_drop, axis=1)\n",
    "    A[numeric_columns] = imputer.fit_transform(A[numeric_columns])\n",
    "    A.dropna(inplace=True)\n",
    "    for col in object_columns:\n",
    "        A[col] = pd.to_datetime(A[col], errors='coerce')\n",
    "        A[f'numeric_{col}'] = A[col].dt.strftime('%Y%m%d').astype(float)\n",
    "        A.drop(col, axis=1, inplace=True)\n",
    "        A.rename(columns={f'numeric_{col}': col}, inplace=True)\n",
    "\n",
    "    for col in numeric_columns:\n",
    "        mean_value = A[col].mean()\n",
    "        std_value = A[col].std()\n",
    "        z_scores = (A[col] - mean_value) / std_value\n",
    "        outlier_indices = np.abs(z_scores) > 3\n",
    "        A.loc[outlier_indices, col] = mean_value\n",
    "\n",
    "    return A\n",
    "\n",
    "def drop_missing_and_outliers(X):\n",
    "    A = X.copy()\n",
    "    A = A.drop(columns=columns_to_drop, axis=1)\n",
    "    A[numeric_columns] = imputer.fit_transform(A[numeric_columns])\n",
    "    A.dropna(inplace=True)\n",
    "    for col in object_columns:\n",
    "        A[col] = pd.to_datetime(A[col], errors='coerce')\n",
    "        A[f'numeric_{col}'] = A[col].dt.strftime('%Y%m%d').astype(float)\n",
    "        A.drop(col, axis=1, inplace=True)\n",
    "        A.rename(columns={f'numeric_{col}': col}, inplace=True)\n",
    "\n",
    "    # Identify and drop outliers from numeric columns\n",
    "    for col in numeric_columns:\n",
    "        mean_value = A[col].mean()\n",
    "        std_value = A[col].std()\n",
    "        z_scores = (A[col] - mean_value) / std_value\n",
    "        outlier_indices = np.abs(z_scores) > 3\n",
    "        A = A.loc[~outlier_indices]\n",
    "\n",
    "    return A\n",
    "\n",
    "\n",
    "\n",
    "# Apply the functions to train and test data\n",
    "merged_train_data_filled = missing_and_outliers(pd.concat([X_train, y_train], axis=1))\n",
    "merged_test_data_filled = missing_and_outliers(pd.concat([X_test, y_test], axis=1))\n",
    "\n",
    "merged_train_data_dropped = drop_missing_and_outliers(pd.concat([X_train, y_train], axis=1))\n",
    "merged_test_data_dropped = drop_missing_and_outliers(pd.concat([X_test, y_test], axis=1))\n",
    "\n",
    "# Prepare the datasets\n",
    "X_train_filled, y_train_filled = merged_train_data_filled.drop('ack', axis=1), merged_train_data_filled['ack']\n",
    "X_test_filled, y_test_filled = merged_test_data_filled.drop('ack', axis=1), merged_test_data_filled['ack']\n",
    "\n",
    "X_train_dropped, y_train_dropped = merged_train_data_dropped.drop('ack', axis=1), merged_train_data_dropped['ack']\n",
    "X_test_dropped, y_test_dropped = merged_test_data_dropped.drop('ack', axis=1), merged_test_data_dropped['ack']\n",
    "\n",
    "# Initialize and evaluate the model\n",
    "gb_model = GradientBoostingClassifier()\n",
    "\n",
    "gb_filled_accuracy, gb_filled_precision, gb_filled_recall = evaluate_model(gb_model, X_train_filled, y_train_filled, X_test_filled, y_test_filled)\n",
    "gb_dropped_accuracy, gb_dropped_precision, gb_dropped_recall = evaluate_model(gb_model, X_train_dropped, y_train_dropped, X_test_dropped, y_test_dropped)\n",
    "\n",
    "\n",
    "print(\"Model with filled missing values and filled outliers:\")\n",
    "print(f\"Accuracy: {gb_filled_accuracy:.5f}, Precision: {gb_filled_precision:.5f}, Recall: {gb_filled_recall:.5f}\")\n",
    "\n",
    "print(\"\\nModel with filled missing values and dropped outliers:\")\n",
    "print(f\"Accuracy: {gb_dropped_accuracy:.5f}, Precision: {gb_dropped_precision:.5f}, Recall: {gb_dropped_recall:.5f}\")\n",
    "\n",
    "# Determine the best model based on accuracy, precision, and recall\n",
    "best_approach = \"Filled missing values and filled outliers\" if gb_filled_accuracy >= gb_dropped_accuracy else \"Filled missing values and dropped outliers\"\n",
    "print(f\"\\nBest Model Approach: {best_approach} - Based on Accuracy\")\n",
    "\n",
    "X_train = X_train_filled if gb_filled_accuracy >= gb_dropped_accuracy else X_train_dropped\n",
    "X_test = X_test_filled if gb_filled_accuracy >= gb_dropped_accuracy else X_test_dropped\n",
    "y_train = y_train_filled if gb_filled_accuracy >= gb_dropped_accuracy else y_train_dropped\n",
    "y_test = y_test_filled if gb_filled_accuracy >= gb_dropped_accuracy else y_test_dropped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vyhodnotenie fillingu missing values a fillingu/dropnutiu outlierov\n",
    "Vyplnenie missing values sme museli vykonať, pretože niektoré algoritmy strojového učenia nepracujú s nimi. Výslendky sme porovnali s výsledkami, ktoré sme dostali pri dropnutí vs fillingu outlierov. Výsledky boli podobné, ale pri fillingu sme dosiahli lepšie výsledky. Preto sme sa rozhodli použiť fillnutie outlierov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, PowerTransformer\n",
    "\n",
    "def scale_data(X):\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = X.copy()\n",
    "    X_scaled[numeric_columns] = scaler.fit_transform(X_scaled[numeric_columns])\n",
    "    return X_scaled\n",
    "\n",
    "def transform_data(X):\n",
    "    transformer = PowerTransformer()\n",
    "    X_transformed = X.copy()\n",
    "    X_transformed[numeric_columns] = transformer.fit_transform(X_transformed[numeric_columns])\n",
    "    return X_transformed\n",
    "\n",
    "def scale_and_transform_data(X):\n",
    "    scaler = MinMaxScaler()\n",
    "    transformer = PowerTransformer()\n",
    "    X_scaled_transformed = X.copy()\n",
    "    X_scaled_transformed[numeric_columns] = scaler.fit_transform(X_scaled_transformed[numeric_columns])\n",
    "    X_scaled_transformed[numeric_columns] = transformer.fit_transform(X_scaled_transformed[numeric_columns])\n",
    "    return X_scaled_transformed\n",
    "\n",
    "def transform_and_scale_data(X):\n",
    "    transformer = PowerTransformer()\n",
    "    scaler = MinMaxScaler()\n",
    "    X_transformed_scaled = X.copy()\n",
    "    X_transformed_scaled[numeric_columns] = transformer.fit_transform(X_transformed_scaled[numeric_columns])\n",
    "    X_transformed_scaled[numeric_columns] = scaler.fit_transform(X_transformed_scaled[numeric_columns])\n",
    "    return X_transformed_scaled\n",
    "\n",
    "# Apply the scaling and transforming functions to your data\n",
    "X_train_scaled = scale_data(X_train)\n",
    "X_test_scaled = scale_data(X_test)\n",
    "\n",
    "X_train_transformed = transform_data(X_train)\n",
    "X_test_transformed = transform_data(X_test)\n",
    "\n",
    "# Apply the combined scaling and transforming function to your data\n",
    "X_train_scaled_transformed = scale_and_transform_data(X_train)\n",
    "X_test_scaled_transformed = scale_and_transform_data(X_test)\n",
    "\n",
    "X_train_transformed_scaled = transform_and_scale_data(X_train)\n",
    "X_test_transformed_scaled = transform_and_scale_data(X_test)\n",
    "\n",
    "# Evaluate model on original data\n",
    "accuracy_original, precision_original, recall_original = evaluate_model(deepcopy(gb_model), X_train, y_train, X_test, y_test)\n",
    "print(f\"Original Data - Accuracy: {accuracy_original:.5f}, Precision: {precision_original:.5f}, Recall: {recall_original:.5f}\")\n",
    "\n",
    "# Evaluate model on scaled data\n",
    "accuracy_scaled, precision_scaled, recall_scaled = evaluate_model(deepcopy(gb_model), X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "print(f\"Scaled Data - Accuracy: {accuracy_scaled:.5f}, Precision: {precision_scaled:.5f}, Recall: {recall_scaled:.5f}\")\n",
    "\n",
    "# Evaluate model on transformed data\n",
    "accuracy_transformed, precision_transformed, recall_transformed = evaluate_model(deepcopy(gb_model), X_train_transformed, y_train, X_test_transformed, y_test)\n",
    "print(f\"Transformed Data - Accuracy: {accuracy_transformed:.5f}, Precision: {precision_transformed:.5f}, Recall: {recall_transformed:.5f}\")\n",
    "\n",
    "# Evaluate model on scaled and transformed data\n",
    "accuracy_scaled_transformed, precision_scaled_transformed, recall_scaled_transformed = evaluate_model(deepcopy(gb_model), X_train_scaled_transformed, y_train, X_test_scaled_transformed, y_test)\n",
    "print(f\"Scaled & Transformed Data - Accuracy: {accuracy_scaled_transformed:.5f}, Precision: {precision_scaled_transformed:.5f}, Recall: {recall_scaled_transformed:.5f}\")\n",
    "\n",
    "accuracy_transformed_scaled, precision_transformed_scaled, recall_transformed_scaled = evaluate_model(deepcopy(gb_model), X_train_transformed_scaled, y_train, X_test_transformed_scaled, y_test)\n",
    "print(f\"Transformed & Scaled Data - Accuracy: {accuracy_transformed_scaled:.5f}, Precision: {precision_transformed_scaled:.5f}, Recall: {recall_transformed_scaled:.5f}\")\n",
    "\n",
    "# Store the results\n",
    "results = {\n",
    "    \"Original Data\": (accuracy_original, precision_original, recall_original),\n",
    "    \"Scaled Data\": (accuracy_scaled, precision_scaled, recall_scaled),\n",
    "    \"Transformed Data\": (accuracy_transformed, precision_transformed, recall_transformed),\n",
    "    \"Scaled & Transformed Data\": (accuracy_scaled_transformed, precision_scaled_transformed, recall_scaled_transformed),\n",
    "    \"Transformed & Scaled Data\": (accuracy_transformed_scaled, precision_transformed_scaled, recall_transformed_scaled)\n",
    "}\n",
    "\n",
    "# Determine the best model\n",
    "best_model = max(results, key=lambda x: (results[x][0], results[x][1], results[x][2]))  # Prioritizing accuracy, then precision, then recall\n",
    "\n",
    "# Print the best model and its metrics\n",
    "best_accuracy, best_precision, best_recall = results[best_model]\n",
    "print(f\"\\nBest Model: {best_model}\")\n",
    "print(f\"Accuracy: {best_accuracy:.5f}, Precision: {best_precision:.5f}, Recall: {best_recall:.5f}\")\n",
    "\n",
    "# Explanation\n",
    "if best_model == \"Original Data\":\n",
    "    explanation = \"This model performs best likely due to the nature of the data, which may not require scaling or transformation for optimal results.\"\n",
    "elif best_model == \"Scaled Data\":\n",
    "    explanation = \"Scaling the data to a uniform range has likely improved model performance by equalizing the influence of all features.\"\n",
    "elif best_model == \"Transformed Data\":\n",
    "    explanation = \"Transforming the data to a more Gaussian distribution has likely improved the model's ability to learn from the data.\"\n",
    "elif best_model == \"Scaled & Transformed Data\":\n",
    "    explanation = \"Scaling and transforming the data have both improved the model's performance, indicating the data benefits from both adjustments.\"\n",
    "else:\n",
    "    explanation = \"Transforming and scaling the data in this order has likely improved the model's performance, indicating the data benefits from both adjustments.\"\n",
    "\n",
    "print(f\"Reason: {explanation}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vyhodnotenie scalingu a transformácie dát"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na našich dátach sme previedli rôzne kombinácie scalingu a transformácie. Použili sme MinMaxScaler a PowerTransformer. Výsledky sme porovnali a zistili, že najlepšie a zároveň rovnaké výsledky sme dosiahli pri transformovaní a scalingu. Preto sme sa rozhodli použiť obe metódy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE, SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "def select_features(method, model, X, y, k):\n",
    "    if method == 'SelectKBest':\n",
    "        selector = SelectKBest(f_classif, k=k)\n",
    "        X_selected = selector.fit_transform(X, y)\n",
    "    elif method == 'RandomForestImportance':\n",
    "        forest = RandomForestClassifier()\n",
    "        forest.fit(X, y)\n",
    "        selector = SelectFromModel(forest, threshold=-np.inf, max_features=k)\n",
    "        X_selected = selector.fit_transform(X, y)\n",
    "\n",
    "    return X_selected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "model_results = {}\n",
    "feature_counts = range(1,4) \n",
    "feature_methods = ['SelectKBest', 'RandomForestImportance']\n",
    "\n",
    "models = {\n",
    "    'GradientBoosting': GradientBoostingClassifier(),\n",
    "    'RandomForest': RandomForestClassifier(),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'LogisticRegression': LogisticRegression(),\n",
    "    'DecisionTree': DecisionTreeClassifier(),\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    for method in feature_methods:\n",
    "        for k in feature_counts:\n",
    "            X_train_selected = select_features(method, model, X_train, y_train, k)\n",
    "            X_test_selected = select_features(method, model, X_test, y_test, k)\n",
    "\n",
    "            # Fit the model on the training data\n",
    "            start_time = time.time()\n",
    "            model.fit(X_train_selected, y_train)\n",
    "            end_time = time.time()\n",
    "\n",
    "            # Predict on both the training and testing data\n",
    "            train_predictions = model.predict(X_train_selected)\n",
    "            test_predictions = model.predict(X_test_selected)\n",
    "\n",
    "            # Calculate accuracy and run-time for training data\n",
    "            train_accuracy = accuracy_score(y_train, train_predictions)\n",
    "\n",
    "            # Calculate accuracy and run-time for testing data\n",
    "            test_accuracy = accuracy_score(y_test, test_predictions)\n",
    "            run_time = end_time - start_time\n",
    "\n",
    "            result_key_train = f\"{name}_{method}_Top{k}_Train\"\n",
    "            result_key_test = f\"{name}_{method}_Top{k}_Test\"\n",
    "\n",
    "            if train_accuracy or test_accuracy > 0.985: # Ignore models with overfitting\n",
    "                continue\n",
    "            else:\n",
    "                model_results[result_key_train] = {'Accuracy_train': train_accuracy, 'Accuracy_test': test_accuracy, 'Run-time': run_time}\n",
    "\n",
    "\n",
    "\n",
    "for item in model_results.items():\n",
    "    print(item)\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"default\", category=ConvergenceWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vyhodnotenie výberu atribútov a algoritmov strojového učenia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Použitá literatúra:\n",
    "- https://github.com/thismlguy/analytics_vidhya/tree/master/Articles/Parameter_Tuning_GBM_with_Example\n",
    "- https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
